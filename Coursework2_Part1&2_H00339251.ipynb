{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F20M/F21ML Coursework 2  Part I and II\n",
    "# Part I: Linear Model with Gradient Descent\n",
    "\n",
    "- In this part you will implement your own Linear Model from scratch without using any third pary machine leanring libraries (see detalied coursework instructions).\n",
    "- You are not allowed to use any existing machine leanring libraries or other open source code,for example, you cannot use **sklearn.linear_model.SGDClassifier**\n",
    "- You can of course use numpy, pandas, and some assitive tools in skllearn, but not the linear classifer library. For example, you cannot use sklearn.linear_model.SGDClassifier. \n",
    "\n",
    "- You will first try your implemented model on the Iris dataset as it is easier to know if your implementation is correct or not. \n",
    "- You will then test the model on a sligthly more challenging dataset. You will perform hyperparameter tuning and select the best model, and then output the test accruracy of your best model. \n",
    "- You need to record and output major information in this notebook, including training, test accuracies.\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the below GD class to be used by the next task (see the next task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "class GD:\n",
    "    def __init__(self, max_iter, num_features, eta=1, lam=1):\n",
    "        super().__init__()\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.num_features = num_features\n",
    "        self.weights=np.zeros(self.num_features)\n",
    "        self.bias=0\n",
    "        self.first=1000000000000000\n",
    "\n",
    "        \n",
    "    def difference(self,second):\n",
    "        #records differences\n",
    "        diff=second-self.first\n",
    "        self.first=second\n",
    "        return diff \n",
    "        \n",
    "     # TODO: complete implementation\n",
    "    def init_parameters(self):\n",
    "        # complete implementation\n",
    "        self.weights=np.zeros(self.num_features)\n",
    "        self.bias=0\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    # TODO: complete implementation\n",
    "    # You are strongly suggested to use related functions in NumPy, including numpy.dot, numpy.square, numpy.sum\n",
    "    # because this will make the implemetation much concise and efficient. \n",
    "    def fit(self, X, y):\n",
    "        # complete implementation, \n",
    "        boolean=np.array([1 if i>0 else 0 for i in y])\n",
    "        self.init_parameters()\n",
    "        # You need to record and output total loss in each iteration, or every 10/100/1000 iterations depending on \n",
    "        # how many iterations you will run.\n",
    "        for k in range(self.max_iter):\n",
    "            self.weights=self.weights-((self.eta)*((((self.lam)*(self.weights))+2*sum(X*((np.dot(X,self.weights)+self.bias)-boolean)[:, np.newaxis]))))\n",
    "            self.bias=self.bias-((self.eta)*(sum((np.dot(X,self.weights)+self.bias)-boolean)*2))\n",
    "            dif=self.difference(second=((sum(((np.dot(X,self.weights)+self.bias)-boolean))**2)+((self.lam/2)*np.dot(self.weights,self.weights))))\n",
    "            if(((sum(((np.dot(X,self.weights)+self.bias)-boolean))**2)+((self.lam/2)*np.dot(self.weights,self.weights)))>1000000000000000000000 or abs(dif)<0.0000000000000000001):\n",
    "                print(\"Algorithm has stopped!\")\n",
    "                break \n",
    "            else:\n",
    "                pass \n",
    "            print((sum(((np.dot(X,self.weights)+self.bias)-boolean))**2)+((self.lam/2)*np.dot(self.weights,self.weights)))\n",
    "           \n",
    "        \n",
    "        print(\"Finished!\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # TODO: complete implementation\n",
    "    \n",
    "    # Again, numpy.dot may help. \n",
    "    def predict(self, X, y=None):\n",
    "        predicted_labels = []\n",
    "        for a, b in enumerate(X):\n",
    "            lin_out=np.dot(b, self.weights)+self.bias\n",
    "            y_hat=np.where(lin_out>0.5,1,0)\n",
    "            predicted_labels.append(y_hat)\n",
    "            \n",
    "        \n",
    "        return predicted_labels\n",
    "            \n",
    "            \n",
    "       \n",
    "         \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your GD implementation above on the  Iris Dataset (to check if your implementation is correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import cross_val_score #\n",
    "IrisDataset= datasets.load_iris()\n",
    "X = IrisDataset.data  # load feature data\n",
    "y = IrisDataset.target # load labels\n",
    "# to make things easier, we only consider two classes, so we load the first 100 records\n",
    "X=X[0:100, :]\n",
    "y=y[0:100]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import datasets\n",
    "IrisDataset= datasets.load_iris()\n",
    "X = IrisDataset.data  # load feature data\n",
    "y = IrisDataset.target # load labels\n",
    "# to make things easier, we only consider two classes, so we load the first 100 records\n",
    "X=X[0:100, :]\n",
    "y=y[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement the GD class\n",
    "\n",
    "# hyperparameters for the gradient descent algorithm\n",
    "max_iter = 10\n",
    "num_features = X_train.shape[1]\n",
    "eta = 0.0001 # eta step (Change default value=0 and choose wisely! Double-check CW instructions)\n",
    "lam = 1 # lambda term for regularisation (also choose wisely!)\n",
    "\n",
    "# Step 1. Initialise model with hyperparameters\n",
    "linear_model = GD(max_iter, num_features, eta, lam)\n",
    "\n",
    "# Step 2. Train model on a subset of the training set (first 10k examples)\n",
    "print(\"Training model for {} max_iter\".format(max_iter))\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3. Compute performance on test set\n",
    "y_pred = linear_model.predict(X_test)  \n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev=train_test_split(X_train, y_train, test_size=0.11)\n",
    "X_train\n",
    "print(X.shape[0])\n",
    "print(X_dev.shape[0])    \n",
    "print(X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO\n",
    "# Step 4: Hyperparameter tuning: max_iter, eta, and lam. \n",
    "max_iter = 10000\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "eta=np.array([1,0.1,0.01,0.001,0.0001])\n",
    "lam=np.linspace(1,100,100)\n",
    "accuracymeaniris=a=np.zeros([5,100])\n",
    "for i in eta:\n",
    "    for j in lam:\n",
    "        linear_model = GD(max_iter, num_features, i, j)\n",
    "        linear_model.fit(X_train, y_train)\n",
    "        y_pred = linear_model.predict(X_dev) \n",
    "        accuracymeaniris[np.where(eta==i)[0][0]][np.where(lam==j)[0][0]]=accuracy_score(y_dev, y_pred)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(accuracymeaniris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 5: output the test accuracy with the best model you have got from hyperparameter tuning. \n",
    "# If this is >80%, or even >90%, it is a good indication of your model being working. \n",
    "max_iter = 10000\n",
    "num_features = X_train.shape[1]\n",
    "eta1=0.0001\n",
    "lam1=1\n",
    "\n",
    "linear_model = GD(max_iter, num_features, eta1, lam1)\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred = linear_model.predict(X_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now test the GD class on the Breast Cancer dataset. \n",
    "\n",
    "- TIPS: the features in the dataset are not in the same scale. The values of some features are several orders of magnitude higher than others. This will affect the learning. You will need to preprocess the input data.\n",
    "\n",
    "- Remember: blindly applying machine learning to a problem without closely examining the data first is unwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Now test the GD class on the Breast Cancer dataset. \n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"C:/Users/leeyz888/OneDrive/Desktop/Statistical Machine Learning/CW2/breast_cancer_data.csv\")\n",
    "\n",
    "df.drop(['id','Unnamed: 32'], inplace=True, axis=1) # remove useless columns\n",
    "df.diagnosis.replace({'B':0,'M':1}, inplace=True)  # binary encoding\n",
    "\n",
    "X = df.drop(['diagnosis'], axis=1)\n",
    "y = df[['diagnosis']].values\n",
    "y=y.reshape ((y.shape[0],))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO: You need to preprocess the dataset as the features are not in the same scale.\n",
    "for a,b in enumerate(X):\n",
    "    X[b]=scaler.fit_transform(X[b].array.reshape(-1, 1))\n",
    "X \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['texture_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.087873)\n",
    "#X_test\n",
    "\n",
    "X_test=X_test.to_numpy()\n",
    "X_train=X_train.to_numpy()\n",
    "   \n",
    "\n",
    "\n",
    "# hyperparameters for the gradient descent algorithm\n",
    "max_iter = 10000\n",
    "eta = 0.0001 # eta step (Change default value=0 and choose wisely! Double-check CW instructions)\n",
    "lam = 1 # lambda term for regularisation (also choose wisely!)\n",
    "\n",
    "# Step 1. Initialise model with hyperparameters\n",
    "num_features = X_train.shape[1]\n",
    "linear_model = GD(max_iter, num_features, eta, lam)\n",
    "\n",
    "# Step 2. Train model on  the training set\n",
    "print(\"Training model for {} max_iter\".format(max_iter))\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 3. Compute performance on test set\n",
    "y_pred = linear_model.predict(X_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Splits into train and dev sets ###\n",
    "X_train, X_dev, y_train, y_dev=train_test_split(X_train, y_train, test_size=0.096)\n",
    "print(X.shape[0])\n",
    "print(X_dev.shape[0])\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO Step 4: perform hyperparameter tuning to improve learning performance\n",
    "### 10000 max iter ###\n",
    "max_iter = 10000\n",
    "eta=np.array([1,0.1,0.01,0.001,0.0001])\n",
    "lam=np.linspace(1,100,100)\n",
    "accuracymean=np.zeros([5,100])\n",
    "# iterate over all poss choices\n",
    "for i in eta:\n",
    "    for j in lam:\n",
    "        linear_model = GD(max_iter, num_features = X_train.shape[1], eta=i,lam=j)\n",
    "        linear_model.fit(X_train, y_train)\n",
    "        y_pred = linear_model.predict(X_dev)\n",
    "        accuracymean[np.where(eta==i)[0][0]][np.where(lam==j)[0][0]]=accuracy_score(y_dev, y_pred)\n",
    "    \n",
    "print(accuracymean)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracymean_10000=accuracymean  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 1000 max iter ###\n",
    "\n",
    "max_iter = 1000\n",
    "eta=np.array([1,0.1,0.01,0.001,0.0001])\n",
    "lam=np.linspace(1,100,100)\n",
    "accuracymean_1000=np.zeros([5,100])\n",
    "# iterate over all poss choices\n",
    "for i in eta:\n",
    "    for j in lam:\n",
    "        linear_model = GD(max_iter, num_features = X_train.shape[1], eta=i,lam=j)\n",
    "        linear_model.fit(X_train, y_train)\n",
    "        y_pred = linear_model.predict(X_dev)\n",
    "        accuracymean_1000[np.where(eta==i)[0][0]][np.where(lam==j)[0][0]]=accuracy_score(y_dev, y_pred)\n",
    "    \n",
    "print(accuracymean_1000)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 5000 max iter ###\n",
    "\n",
    "max_iter = 5000\n",
    "eta=np.array([1,0.1,0.01,0.001,0.0001])\n",
    "lam=np.linspace(1,100,100)\n",
    "accuracymean_5000=np.zeros([5,100])\n",
    "# iterate over all poss choices\n",
    "for i in eta:\n",
    "    for j in lam:\n",
    "        linear_model = GD(max_iter, num_features = X_train.shape[1], eta=i,lam=j)\n",
    "        linear_model.fit(X_train, y_train)\n",
    "        y_pred = linear_model.predict(X_dev)\n",
    "        accuracymean_5000[np.where(eta==i)[0][0]][np.where(lam==j)[0][0]]=accuracy_score(y_dev, y_pred)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots showing the accuracy for each hyperparameter ###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "x=range(1,101)\n",
    "line1=plt.plot(x,accuracymean_1000[3],linestyle='-',label='For eta=0.001')  \n",
    "line2=plt.plot(x,accuracymean_1000[4],linestyle='-',label='For eta=0.0001') \n",
    "line3=plt.plot(x,accuracymean_1000[2],linestyle='-',label='For eta=1,0.1&0.01') \n",
    "plt.title('For 1000 max iter')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "x=range(1,101)\n",
    "line1=plt.plot(x,accuracymean_5000[3],linestyle='-',label='For eta=0.001')  \n",
    "line2=plt.plot(x,accuracymean_5000[4],linestyle='-',label='For eta=0.0001')  \n",
    "line3=plt.plot(x,accuracymean_5000[2],linestyle='-',label='For eta=1,0.1&0.01') \n",
    "plt.title('For 5000 max iter')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "x=range(1,101)\n",
    "line1=plt.plot(x,accuracymean_10000[3],linestyle='-',label='For eta=0.001')  \n",
    "line2=plt.plot(x,accuracymean_10000[4],linestyle='-',label='For eta=0.0001') \n",
    "line3=plt.plot(x,accuracymean_10000[2],linestyle='-',label='For eta=1,0.1&0.01') \n",
    "plt.title('For 10000 max iter')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "# Step 5: report the test accuracy for your best model\n",
    "\n",
    "max_iter = 1000\n",
    "eta = 0.001 # eta step (Change default value=0 and choose wisely! Double-check CW instructions)\n",
    "lam = 2 # lambda term for regularisation (also choose wisely!)\n",
    "\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "linear_model = GD(max_iter, num_features, eta, lam)\n",
    "\n",
    "\n",
    "print(\"Training model for {} max_iter\".format(max_iter))\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred=linear_model.predict(X_test)\n",
    "\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize) #print long numpy array\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Implement Neural Network from Scratch\n",
    "\n",
    "- In this part, you will implement a neural network with only ONE hidden layer from scratch using Python. You will implement both forward pass and backpropagation and test the algorithm on the sonar dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import softmax as sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, num_features, num_hidden1 ,alpha,alpha1, max_epochs, num_output, _EPSILON, epsilon2):\n",
    "        super().__init__()\n",
    "        self.num_features=num_features  # number of input nodes (features)\n",
    "        self.num_hidden1=num_hidden1  # number of hidden nodes for 1st hidden layer\n",
    "        self.alpha=alpha  # learning rate\n",
    "        self.alpha1=alpha1 # alpha for para RelU\n",
    "        self.max_epochs=max_epochs # maximum number of epochs\n",
    "        self.num_output=num_output # number of output nodes\n",
    "        self._EPSILON=_EPSILON\n",
    "        self.eps=epsilon2 # for gradient checking \n",
    "        self.loss = [] #list to store losses per 100 epochs \n",
    "        self.trainingaccur=[] # list to store training accuracy per 100 epochs \n",
    "        self.devaccur=[]\n",
    "        self.Weights_Input_to_H1=np.random.randn(self.num_hidden1, self.num_features)*(0.1)\n",
    "        self.Bias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.Weights_H1_to_output=np.random.randn(self.num_output, self.num_hidden1)*(0.1)\n",
    "        self.Bias_H1_to_output=np.zeros([self.num_output,1])\n",
    "        self.dWeights_Input_to_H1=np.zeros([self.num_hidden1, self.num_features])\n",
    "        self.dBias_Input_to_H1=np.zeros([self.num_hidden1,1])\n",
    "        self.dWeights_H1_to_output=np.zeros([self.num_output, self.num_hidden1])\n",
    "        self.dBias_H1_to_output=np.zeros([self.num_output,1])\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def relU(self,X):\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def Para_relU(self,alpha,X):\n",
    "        return np.maximum(X,0)+((alpha)*np.minimum(X,0))\n",
    "\n",
    "    def Para_deriv_wrt_X(self,alpha,X):\n",
    "        \n",
    "        return np.where(X<=0,alpha,1)\n",
    "        \n",
    "\n",
    "    def Para_deriv_wrt_alpha(self,X):\n",
    "        return np.where(X<=0,X,0)\n",
    "    \n",
    "    def tanh(self,X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def deriv_tanh(self,X):\n",
    "        return 1-((np.tanh(X))**2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def deriv(self,X):\n",
    "        return np.where(X<=0,0,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        return np.exp(x - np.max(x, axis=0)) / np.sum(np.exp(x - np.max(x, axis=0)), axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    # TODO: complete implementation for forward pass\n",
    "    def forward(self, X):\n",
    "        self.z1=np.dot((self.Weights_Input_to_H1),(X))+self.Bias_Input_to_H1\n",
    "        self.a1=self.tanh(self.z1)\n",
    "        self.z2=np.dot((self.Weights_H1_to_output),(self.a1))+self.Bias_H1_to_output\n",
    "        self.a2=((self.z2))\n",
    "        return self.a2\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # TODO: complete implementation for backpropagation\n",
    "    # the following Numpy functions may be useful: np.dot, np.sum, np.tanh, numpy.ndarray.T\n",
    "    def backprop(self, X, t):\n",
    "      \n",
    "        self.dz2=(self.a2.reshape(self.num_output,-1)-t.reshape(self.num_output,-1))/((X.shape[1]*X.shape[0])*0.5)\n",
    "        self.dBias_H1_to_output=np.sum(self.dz2,axis=1,keepdims=True)\n",
    "        self.dWeights_H1_to_output=np.dot((self.dz2),self.a1.T)\n",
    "        self.dz1=(np.dot(self.Weights_H1_to_output.T,self.dz2)) * (self.deriv_tanh(self.z1))\n",
    "        self.dalpha1=np.sum((np.dot(self.Weights_H1_to_output.T,self.dz2)) * (self.Para_deriv_wrt_alpha(self.z1)))\n",
    "        self.dBias_Input_to_H1=np.sum(self.dz1,axis=1,keepdims=True)\n",
    "        self.dWeights_Input_to_H1=np.dot((self.dz1),X.T)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "              \n",
    "                        \n",
    "                \n",
    "      \n",
    "        \n",
    "        \n",
    "    \n",
    "    #TODO: complete implementation for fitting data, and change the existing code if needed\n",
    "    def fit(self, x_train_data, y_train_data,x_dev_data,y_dev_data):\n",
    "       \n",
    "        \n",
    "        \n",
    "        for step in range(self.max_epochs):\n",
    "            self.forward((x_train_data))\n",
    "            self.backprop((x_train_data),(y_train_data))\n",
    "            self.Bias_H1_to_output=self.Bias_H1_to_output-((self.alpha)*(self.dBias_H1_to_output))\n",
    "            self.Weights_H1_to_output=self.Weights_H1_to_output-((self.alpha)*(self.dWeights_H1_to_output))\n",
    "            self.Bias_Input_to_H1=self.Bias_Input_to_H1-((self.alpha)*(self.dBias_Input_to_H1))\n",
    "            self.Weights_Input_to_H1=self.Weights_Input_to_H1-((self.alpha)*(self.dWeights_Input_to_H1))\n",
    "            self.alpha1=self.alpha1-((self.alpha)*(self.dalpha1))\n",
    "            \n",
    "\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                self.MSEs=np.mean((self.dz2*((x_train_data.shape[1]*x_train_data.shape[0])*0.5))**2)\n",
    "                self.trainingaccuracy=accuracy_score(np.argmax(y_train_data,axis=0),np.argmax(self.forward(x_train_data),axis=0))\n",
    "                self.devaccuracy=accuracy_score(np.argmax(y_dev_data,axis=0),np.argmax(self.forward(x_dev_data),axis=0))\n",
    "                print(f'step: {step},  loss: {self.MSEs:3.150f}') \n",
    "                print(accuracy_score(np.argmax(y_train_data,axis=0),np.argmax(self.forward(x_train_data),axis=0)))\n",
    "                print(accuracy_score(np.argmax(y_dev_data,axis=0),np.argmax(self.forward(x_dev_data),axis=0)))\n",
    "                print(self.alpha1,self.dalpha1)\n",
    "                self.loss.append(self.MSEs)\n",
    "                self.trainingaccur.append(self.trainingaccuracy)\n",
    "                self.devaccur.append(self.devaccuracy)\n",
    "              \n",
    "             \n",
    "               \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "          \n",
    "              \n",
    "            \n",
    "            \n",
    "    def predict(self,X,y=None):\n",
    "        self.forward(X)\n",
    "        if(self.num_output>1):\n",
    "            y_hat=np.argmax(self.a2, axis=0)\n",
    "            temp=accuracy_score(y_hat,np.argmax(y,axis=0))\n",
    "        else:\n",
    "            y_hat=np.where(self.a2>0.5,1,0)\n",
    "            temp=accuracy_score(y_hat,y)\n",
    "        return temp,y_hat\n",
    "\n",
    "    def gradient_checking(self,eps,x_train_data,y_train_data):\n",
    "      epsilon=eps\n",
    "      self.alpha1=self.alpha1+epsilon\n",
    "      self.CCloss_gra_new_upper=log_loss(np.transpose(y_train_data),np.transpose(self.forward(x_train_data)),eps=self._EPSILON,normalize=True)\n",
    "      self.alpha1=self.alpha1-(2*(epsilon))\n",
    "      self.CCloss_gra_new_lower=log_loss(np.transpose(y_train_data),np.transpose(self.forward(x_train_data)),eps=self._EPSILON,normalize=True)\n",
    "      gra=((self.CCloss_gra_new_upper)-(self.CCloss_gra_new_lower))/(2*(epsilon))\n",
    "      self.alpha1=self.alpha1+epsilon\n",
    "\n",
    "      return gra\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the neural network on the sonar dataset\n",
    "- First load the sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_original = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "    header=None)\n",
    "dataset = df_original.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "Y = dataset[:, 60]\n",
    "# convert string labels to binary\n",
    "Y[Y == 'R'] = 0\n",
    "Y[Y == 'M'] = 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=0.11)\n",
    "print(X_train)\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np.array(pd.get_dummies(np.array(Y_train)))\n",
    "Y_dev=np.array(pd.get_dummies(np.array(Y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.transpose(X_train)\n",
    "X_dev=np.transpose(X_dev)\n",
    "Y_train=np.transpose(Y_train)\n",
    "Y_dev=np.transpose(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Y_dev,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.a2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- second, call the neural network with some hyperparameter values, and perform systematic hyperparamter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "numHidden1 = 10 # number of hidden nodes\n",
    "num_features = X_train.shape[0]\n",
    "numOutput = Y_train.shape[0]\n",
    "max_epoches = 100000\n",
    "alpha = 0.1\n",
    "alpha1=0.003\n",
    "epsilon=0.00000000001\n",
    "epsilon_gradient_checking=0.00000000001\n",
    "NN = NeuralNet(num_features, numHidden1, alpha,alpha1, max_epoches, numOutput,epsilon,epsilon_gradient_checking)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train the neural network and predict on the test dataset. Change the code in this step as needed.\n",
    "\n",
    "NN.fit(X_train,Y_train,X_dev,Y_dev)\n",
    "# predict on the test set\n",
    "#Y_pred = NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(NN.forward(X_test.T),axis=0)==Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#Y_pred=NN.predict(X_test.T)\n",
    "#print(Y_pred)\n",
    "print(Y_test)\n",
    "np.mean(((Y_test)==np.argmax(NN.forward(X_test.T),axis=0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 3: Hyperparameter tuning: perform tuning systematically and output plots.\n",
    "eta=np.array([0.1,0.05,0.01,0.001,0.0001])\n",
    "ite=np.array([5000,10000,20000])\n",
    "hidden=np.arange(1,101)\n",
    "accuracy=np.zeros([3,5,100])\n",
    "\n",
    "for i in eta:\n",
    "    for j in ite:\n",
    "        for k in hidden:\n",
    "            NN= NN = NeuralNet(X_train.shape[0], k, i,0.003, j, Y_train.shape[0],0.00000000001,0.0001)\n",
    "            NN.fit(X_train,Y_train,X_dev,Y_dev)\n",
    "            accur,Y_pred = NN.predict(X_dev,Y_dev)\n",
    "            accuracy[np.where(ite==j)[0][0]][np.where(eta==i)[0][0]][np.where(hidden==k)[0][0]]=accur\n",
    "            print(i)\n",
    "            print(j)\n",
    "            print(k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for constructing data frames###\n",
    "hidden_nodes=[]\n",
    "\n",
    "for i in hidden:\n",
    "    hidden_nodes.append(\"With \"+str(i)+\" hidden nodes\")\n",
    "    \n",
    "hidden_nodes\n",
    "\n",
    "eta_string=[]\n",
    "\n",
    "for j in eta:\n",
    "    eta_string.append(\"With \"+str(j)+\" as eta\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct data frames for 500 max iter###\n",
    "import pandas as pd\n",
    "df0=pd.DataFrame(accuracy[0],index=eta_string,columns=hidden_nodes)\n",
    "import dataframe_image as dfi\n",
    "dfi.export(df0, 'dataframe 500 max iter.png')\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct data frames for 1000 max iter###\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame(accuracy[1],index=eta_string,columns=hidden_nodes)\n",
    "import dataframe_image as dfi\n",
    "dfi.export(df1, 'dataframe 1000 max iter.png')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct data frames for 2000 max iter###\n",
    "import pandas as pd\n",
    "df2=pd.DataFrame(accuracy[2],index=eta_string,columns=hidden_nodes)\n",
    "import dataframe_image as dfi\n",
    "dfi.export(df2, 'dataframe 2000 max iter.png')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting for 500 max iter, for first 3 etas ###\n",
    "x=range(1,21)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "line1=plt.plot(x,accuracy[0][0],linestyle='solid',label='For eta=0.1')\n",
    "line2=plt.plot(x,accuracy[0][1],linestyle='dotted',label='For eta=0.05')  \n",
    "line3=plt.plot(x,accuracy[0][2],linestyle='dashed',label='For eta=0.01')   \n",
    "line4=plt.plot(x,accuracy[0][3],linestyle='dashdot',label='For eta=0.001 and 0.0001 ')  \n",
    "plt.title('For 500 max iter')\n",
    "plt.xlabel('no of hidden nodes')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='center', shadow=True)\n",
    "plt.show\n",
    "\n",
    "x=range(1,21)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "line1=plt.plot(x,accuracy[1][0],linestyle='solid',label='For eta=0.1',color='blue')\n",
    "line2=plt.plot(x,accuracy[1][1],linestyle='dotted',label='For eta=0.05')  \n",
    "line3=plt.plot(x,accuracy[1][2],linestyle='dashed',label='For eta=0.01')   \n",
    "line4=plt.plot(x,accuracy[1][3],linestyle='dashdot',label='For eta=0.001')\n",
    "line5=plt.plot(x,accuracy[1][4],linestyle='-',label='For eta=0.0001')\n",
    "plt.title('For 1000 max iter')\n",
    "plt.xlabel('no of hidden nodes')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.show\n",
    "\n",
    "### Plotting for 2000 max iter, first 3 etas \n",
    "x=range(1,21)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "line1=plt.plot(x,accuracy[2][0],linestyle='solid',label='For eta=0.1',color='blue')\n",
    "line2=plt.plot(x,accuracy[2][1],linestyle='dotted',label='For eta=0.05')  \n",
    "line3=plt.plot(x,accuracy[2][2],linestyle='dashed',label='For eta=0.01')   \n",
    "line4=plt.plot(x,accuracy[2][3],linestyle='dashdot',label='For eta=0.001') \n",
    "line5=plt.plot(x,accuracy[2][4],linestyle='-',label='For eta=0.0001')   \n",
    "plt.title('For 2000 max iter')\n",
    "plt.xlabel('no of hidden nodes')\n",
    "plt.ylabel('Dev accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 4: Get the best neural network model and predict on the test set\n",
    "### Best model is on 2000 max iter, 8 hidden nodes and with eta=0.1\n",
    "\n",
    "NN= NeuralNet(X_train.shape[1], 8, 0.1, 2000, 1)\n",
    "NN.fit(X_train,Y_train)\n",
    "Y_pred = NN.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test accuracy of best model ###\n",
    "print(accuracy_score(Y_test.astype(int), Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
