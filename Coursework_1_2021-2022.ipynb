{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part I - Dataset Analysis  (10%)\n",
    "\n",
    "1. Import the **training set ONLY** using Pandas (**HINT**: the dataset is in csv format, and the training set is called bank_train). We have given you the `feature_names` already below.\n",
    "\n",
    "2. Report the type of every feature (**HINT**: `DataFrame.dtype` might be handy).\n",
    "\n",
    "3. Report if the dataset is balanced.\n",
    "\n",
    "4. Print the feature values of every feature. In the case of a numerical feature print the range and average (with 3 decimal points) instead.\n",
    "\n",
    "5. Using plots of histograms, bar-graphs or heatmaps *briefly* comment on the following (don't forget to include the plots in your .ipynb notebook):\n",
    "    - Does the `age` feature follow a normal distribution (just by eyeballing)?\n",
    "    - Is the `poutcome` feature unimodal (just by eyeballing)?\n",
    "    - Is the `education` feature unimodal (just by eyeballing)?\n",
    "    - Taking into account **only** the numerical features do you notice any correlation between pairs of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feature_names = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\n",
    "                              \"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\", \"category\"]\n",
    "\n",
    "# TODO: import the data (Question 1)\n",
    "\n",
    "\n",
    "# TODO: Fill-in (Answer questions 2-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO Fill-in (Answer plot-based questions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Decision Trees  (20%)\n",
    "\n",
    "- Load data without any preprocessing\n",
    "    - We will partition the dataset into two sets: training (we call bank_trainCV) and test dataset (bank_test).\n",
    "- Perform hyperparameter tuning on depth of DT (training on train dataset, and evaluating on the test set)\n",
    "    - Use 10 fold cross validation for the DT with each hyperparameter value\n",
    "    - Plot the mean training accuracy curve for each hyperparameter value (x-axis: depth of DT, y-axis: accuracy), also output these results in the notebook. \n",
    "    - Pick the depth with maximum mean training accuracy. The maximum training accuracy is expected to be more than `65%`\n",
    "- Test on test set ONCE, and report your test accuracy. \n",
    "\n",
    "- Tips: \n",
    "    - you may find the following information useful: Cross Validation with sklearn:  https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "    - The mean accuracy of a model with specific hyperparameter values in 10-fold CV can be retrieved using sklearn.model_selection.cross_val_score\n",
    "    - Avoid data leakage: https://en.wikipedia.org/wiki/Leakage_(machine_learning)#Training_example_leakage  You should never try to peek into the test set. The test set is used only for test, and it is not for guiding the hyperparameter tuning or other ways of improving the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bank dataset from file bank_trainCV\n",
      "Dataset correctly loaded\n",
      "Number of features is 16\n",
      "Number of instances is 8814\n",
      "one-hot selected\n",
      "Loading bank dataset from file bank_test\n",
      "Dataset correctly loaded\n",
      "Number of features is 16\n",
      "Number of instances is 2938\n",
      "one-hot selected\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading, please don't change the code in this cell\n",
    "from BankDataset import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('bank_trainCV', preprocess_onehot=True) # training dataset for cross validation\n",
    "test_dataset = load_dataset('bank_test', preprocess_onehot=True)  # test dataset for final evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import seaborn as sns   # used for plotting\n",
    "import matplotlib.pyplot as plt    # used for plotting\n",
    "from sklearn.model_selection import cross_val_score # This is used for cross validation \n",
    "\n",
    "\n",
    "# TODO: Fill-in (Hyperparameter Tuning with 10 fold cross validation, and plot curves)\n",
    "\n",
    "\n",
    "# TODO: Fill-in (Test on test set) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: k Nearest Neighbours (20%)\n",
    "\n",
    "- Load data with 1-hot encoding and scaling. This time we will use train/dev/test split. We train the model on bank_train, perform hyperparameter tunning on bank_dev, and report test accuracy on bank_test.\n",
    "- Perform hyperparameter tuning on number of neighbours k (training on train, computing accuarcy on dev)\n",
    "- Plot train, dev accuracy curves in a single plot.\n",
    "- Pick k with maximum dev accuracy\n",
    "- Test on test set ONCE, and report your test accuracy\n",
    "- Briefly explain how the train/dev/test scheme differs from using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading (load data again! Careful with preprocessing parameters)\n",
    "# This time you will load bank_train, bank_dev, and bank_test as training, validation, and test datasets, respectively.\n",
    "# You will train the model with bank_train, use the validation set to decide the best hyperparameter, \n",
    "# and finally, use bank_test to evaluate the model. \n",
    "\n",
    "from BankDataset import load_dataset\n",
    "\n",
    "# TODO: Fill-in\n",
    "train_dataset = \n",
    "dev_dataset = \n",
    "test_dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Fill-in (Hyperparameter Tuning, plot curves)\n",
    "\n",
    "\n",
    "# TODO: test on test set once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Perceptron  (25%)\n",
    "\n",
    "- Load data with 1-hot encoding and scaling\n",
    "- Use 10-fold cross vaildation (CV) to perform hyperparameter tuning on the maximum number of iterations and learning rate at the same time. This means you will try several combinations of hyperparameter values and select the best model (meaning the one with the highest mean accuracy in 10-fold CV).\n",
    "- For each combination of hyperparameters, output the mean accuracy of the model in 10-fold CV. \n",
    "- Plot a 3D graph, where x-axis and y-axis show the hyperparameter choices, and z-axis shows the mean accuracy of 10-fold CV. You may find the following links useful when ploting a 3D graph: https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html#bar-plots\n",
    "https://stackoverflow.com/questions/11409690/plotting-a-2d-array-with-mplot3d\n",
    "- Test the best model on test set ONCE, and report your result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading (load data again! Careful with preprocessing parameters)\n",
    "from BankDataset import load_dataset\n",
    "\n",
    "# TODO: Fill-in.  Load bank_trainCV and bank_test as training and test datasets\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill-in. Hyperparameter tuning using 10 fold cross validation on Bank_trainCV dataset.\n",
    "# Then test the selected best model on bank_test dataset. \n",
    "# You should output the performance of each model (with individual values of learning rates and iterations) in 10-fold CV\n",
    "# You should draw a 3-D graph to visualise the above results. \n",
    "# Report the accuracy of the selected model on the test set. \n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d   # You can use this to draw 3D graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score # for cross validation, \n",
    "                                                    #and retrieving mean accuracy with this function\n",
    "\n",
    "# Tips: carefully chose the maximum number of iterations and learning rate. \n",
    "# The mean accuracy of a model with specific hyperparameter values in 10-fold CV can be retrieved using cross_val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V (F20ML only, F21ML students please skip this and go for the next Part): Implement your own Perceptron for Binary Classification (25%)\n",
    "\n",
    "- In this section you will implement your own Perceptron from scratch without using any third-party machine learning libraries or other open source code.\n",
    "- You can of course use numpy, pandas, and some assistive functions in scikit-learn, but not the core machine learning components in scikit-learn. For example, you cannot use **\"sklearn.linear_model.Perceptron\"**\n",
    "- You will first use a very simple Iris dataset to test if your implementation works. \n",
    "- Then you will test your implemented perceptron on the bank dataset as before and perform hyperparameter tuning. \n",
    "- You need to record and output major information in this notebook, including training and test accuracy.\n",
    "- You will produce a similar 3D plot showing training and validation accuracies as in Part IV. \n",
    "- TIPS: Use NumPy will make the implementation much easier and quicker.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "IrisDataset= datasets.load_iris()    # load the Iris Dataset. This is a \"toy\" dataset in Machine Learning.\n",
    "X = IrisDataset.data  # load feature data\n",
    "y = IrisDataset.target # load labels\n",
    "# To make things easier, we only consider two classes, so we load the first 100 records. \n",
    "# You can explore this dataset yourself and see what are in there. \n",
    "X=X[0:100, :]\n",
    "y=y[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define your own Perceptron class here. \n",
    "\n",
    "# TODO: complete the following functions in the Perceptron class, and add more funtions in the class as needed.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# NumPy will be very useful as it makes the code more concise and efficient.\n",
    "# Consider using functions like np.dot, np.zeros\n",
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, no_of_inputs, iterations=100, learning_rate=0.01): # no_of_inputs: number of features\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def predict(self, inputs):  # make predictions for input\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def fit(self, training_inputs, labels): # train the model with training data\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test your implemented Perceptron on the Iris dataset. \n",
    "# If your Perceptron class above is implemented properly, you should get a very\n",
    "# good test accuracy, or even 100%, for Iris dataset. \n",
    "#If the performance is too poor, it is very likely that something is wrong with your implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "perceptron=Perceptron(X.shape[1], iterations=1, learning_rate=0.01)  # wisely choose hyperparameters here.\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred=perceptron.predict(X_test)   \n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"test accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))   \n",
    "# It is expected that you get a accuracy of 80% and above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try loading the bank dataset and test your implemented perceptron on this dataset. \n",
    "from BankDataset import load_dataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Fill-in.  \n",
    "# Load bank_train, bank_dev and bank_test as training, development, and test datasets. \n",
    "\n",
    "\n",
    "# TODO: Hyperparameter tuning.\n",
    "#  Train the model on the training set. \n",
    "#  Tune the hyperparmeters (maxium iterations and learning rate) on the development set \n",
    "#  Output the best model and its accurarcy on the test set\n",
    "#  You should output important information during hyperparameter tuning process, including training and validation \n",
    "# accuracy.\n",
    "# You should plot a 3D graph as in Part IV. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V (F21ML only, F20ML students please ignore this part): Implement your own Perceptron for classification with more than two labels (25%)\n",
    "- In this section you will implement your own Perceptron from scratch without using any third-party machine learning libraries or other open source code.\n",
    "- You can of course use numpy, pandas, and some assistive functions in scikit-learn, but not the core machine learning components in scikit-learn. For example, you cannot use **\"sklearn.linear_model.Perceptron\"**\n",
    "- You will first use a very simple Iris dataset to test if your implementation works. \n",
    "- Then you will test your implemented perceptron on the bank dataset as before and perform hyperparameter tuning. \n",
    "- You need to record and output major information in this notebook, including training and test accuracy.\n",
    "- You will produce a similar 3D plot showing training and validation accuracies as in Part IV. \n",
    "- TIPS: Use NumPy will make the implementation much easier and quicker.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "IrisDataset= datasets.load_iris()\n",
    "X = IrisDataset.data  # load feature data\n",
    "y = IrisDataset.target # load labels\n",
    "# Note we have three classes in the dataset, so we need to extend the original perceptron algorithms for dealing with\n",
    "# both binary classification and multiclass classification (when there are more than two classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your own Perceptron class here. \n",
    "\n",
    "# TODO: complete the following functions in the Perceptron class, and add more funtions in the class as needed.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# NumPy will be very useful as it makes the code more concise and efficient.\n",
    "# Consider using functions like np.dot, np.zeros\n",
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, no_of_inputs, iterations=100, learning_rate=0.01): # no_of_inputs: number of features\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def predict(self, inputs):  # make predictions for input\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def fit(self, training_inputs, labels): # train the model with training data\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1]\n",
      "accuracy is 1.000\n"
     ]
    }
   ],
   "source": [
    "# Test your implemented Perceptron on the Iris dataset. \n",
    "# If your Perceptron class above is implemented properly, you should get a very\n",
    "# good test accuracy, or even 100%, for Iris dataset. \n",
    "#If the performance is too poor, it is very likely that something is wrong with your implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "perceptron=Perceptron(X.shape[1], iterations=1, learning_rate=0.01)  # wisely choose hyperparameters here.\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred=perceptron.predict(X_test)   \n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"test accuracy is {:1.3f}\".format(accuracy_score(y_test, y_pred)))   \n",
    "# It is expected that you get a accuracy of 80% and above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try loading the bank dataset and test your implemented perceptron on this dataset. \n",
    "from BankDataset import load_dataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Fill-in.  \n",
    "# Load bank_train, bank_dev and bank_test as training, development, and test datasets. \n",
    "\n",
    "\n",
    "# TODO: Hyperparameter tuning.\n",
    "#  Train the model on the training set. \n",
    "#  Tune the hyperparmeters (maxium iterations and learning rate) on the development set \n",
    "#  Output the best model and its accurarcy on the test set\n",
    "#  You should output important information during hyperparameter tuning process, including training and validation \n",
    "# accuracy.\n",
    "# You should plot a 3D graph as in Part IV. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
